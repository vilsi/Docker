Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers.[6] Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels.[7] Because all of the containers share the services of a single operating system kernel, they use fewer resources than virtual machines.[8]

The service has both free and premium tiers. The software that hosts the containers is called Docker Engine.[8] It was first started in 2013 and is developed by Docker, Inc.[9]


Contents
1	Operation
1.1	Components
1.2	Tools
2	History
2.1	Adoption
3	See also
4	References
5	External links
Operation

Docker can use different interfaces to access virtualization features of the Linux kernel.[10]
Docker can package an application and its dependencies in a virtual container that can run on any Linux, Windows, or macOS computer. This enables the application to run in a variety of locations, such as on-premises, in a public cloud, and/or in a private cloud.[11] When running on Linux, Docker uses the resource isolation features of the Linux kernel (such as cgroups and kernel namespaces) and a union-capable file system (such as OverlayFS)[12] to allow containers to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines.[13] Docker on macOS uses a Linux virtual machine to run the containers.[14]

Because Docker containers are lightweight, a single server or virtual machine can run several containers simultaneously.[15] A 2018 analysis found that a typical Docker use case involves running eight containers per host, and that a quarter of analyzed organizations run 18 or more per host.[16]

The Linux kernel's support for namespaces mostly[17] isolates an application's view of the operating environment, including process trees, network, user IDs and mounted file systems, while the kernel's cgroups provide resource limiting for memory and CPU.[18] Since version 0.9, Docker includes its own component (called "libcontainer") to use virtualization facilities provided directly by the Linux kernel, in addition to using abstracted virtualization interfaces via libvirt, LXC and systemd-nspawn.[19][10][11][20]

Docker implements a high-level API to provide lightweight containers that run processes in isolation.[21] Docker containers are standard processes, so it is possible to use kernel features to monitor their execution -- including for example the use of tools like strace to observe and intercede with system calls.[22]

Components
The Docker software as a service offering consists of three components:

Software: The Docker daemon, called dockerd, is a persistent process that manages Docker containers and handles container objects. The daemon listens for requests sent via the Docker Engine API.[23][24] The Docker client program, called docker, provides a command-line interface, CLI, that allows users to interact with Docker daemons.[23][25]
Objects: Docker objects are various entities used to assemble an application in Docker. The main classes of Docker objects are images, containers, and services.[23]
A Docker container is a standardized, encapsulated environment that runs applications.[26] A container is managed using the Docker API or CLI.[23]
A Docker image is a read-only template used to build containers. Images are used to store and ship applications.[23]
A Docker service allows containers to be scaled across multiple Docker daemons. The result is known as a swarm, a set of cooperating daemons that communicate through the Docker API.[23]
Registries: A Docker registry is a repository for Docker images. Docker clients connect to registries to download ("pull") images for use or upload ("push") images that they have built. Registries can be public or private. Two main public registries are Docker Hub and Docker Cloud. Docker Hub is the default registry where Docker looks for images.[23][27] Docker registries also allow the creation of notifications based on events.[28]
Tools
Docker Compose is a tool for defining and running multi-container Docker applications.[29] It uses YAML files to configure the application's services and performs the creation and start-up process of all the containers with a single command. The docker-compose CLI utility allows users to run commands on multiple containers at once, for example, building images, scaling containers, running containers that were stopped, and more.[30] Commands related to image manipulation, or user-interactive options, are not relevant in Docker Compose because they address one container.[31] The docker-compose.yml file is used to define an application's services and includes various configuration options. For example, the build option defines configuration options such as the Dockerfile path, the command option allows one to override default Docker commands, and more.[32] The first public beta version of Docker Compose (version 0.0.1) was released on December 21, 2013.[33] The first production-ready version (1.0) was made available on October 16, 2014.[34]
Docker Swarm provides native clustering functionality for Docker containers, which turns a group of Docker engines into a single virtual Docker engine.[35] In Docker 1.12 and higher, Swarm mode is integrated with Docker Engine.[36] The docker swarm CLI[37] utility allows users to run Swarm containers, create discovery tokens, list nodes in the cluster, and more.[38] The docker node CLI utility allows users to run various commands to manage nodes in a swarm, for example, listing the nodes in a swarm, updating nodes, and removing nodes from the swarm.[39] Docker manages swarms using the Raft consensus algorithm. According to Raft, for an update to be performed, the majority of Swarm nodes need to agree on the update.[40][41]
Docker Volume If you copy or create a file in a container, when you stop that container that file (and any other files created or copied) will be deleted.
